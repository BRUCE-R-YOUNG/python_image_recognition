{"cells":[{"cell_type":"markdown","metadata":{"id":"UDnb4-lfp3ya"},"source":["# Pythonで学ぶ画像認識　第6章 画像キャプショニング\n","## 第6.4節 アテンション機構による手法〜Show, attend and tellを実装してみよ"]},{"cell_type":"markdown","metadata":{"id":"oCSbLryKkAUO"},"source":["###モジュールのインポートとGoogleドライブのマウント"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8mxL-PsjRXaT"},"outputs":[],"source":["import os\n","import glob\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","import datetime\n","from tqdm import tqdm\n","import pickle\n","import random\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from pycocotools.coco import COCO\n","from PIL import Image\n","import skimage.transform\n","from typing import Sequence, Dict, Tuple, Union\n","from collections import deque\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pack_padded_sequence\n","from torchvision import models\n","import torchvision.transforms as T\n","import torchvision.datasets as dataset\n","from torch.utils.data.dataset import Subset\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.append('drive/MyDrive/python_image_recognition/6_img_captioning/6_4_show_attend_and_tell')\n","\n","import util"]},{"cell_type":"markdown","metadata":{"id":"yuymkMNhgRWJ"},"source":["### エンコーダの実装"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zd2Zcbc59rZD"},"outputs":[],"source":["class CNNEncoder(nn.Module):\n","    '''\n","    Show, attend and tellのエンコーダ\n","    encoded_img_size: 画像部分領域サイズ\n","    '''\n","    def __init__(self, encoded_img_size: int):\n","        super().__init__()\n","\n","        # IMAGENET1K_V2で事前学習された\n","        # ResNet152モデルをバックボーンとする\n","        resnet = models.resnet152(weights=\"IMAGENET1K_V2\") \n","\n","        # AdaptiveAvgPool2dで部分領域(14x14)を作成        \n","        resnet.avgpool = nn.AdaptiveAvgPool2d(encoded_img_size)\n","\n","        # 特徴抽出器として使うため全結合層を削除\n","        modules = list(resnet.children())[:-1]\n","        self.backbone = nn.Sequential(*modules)\n","\n","    '''\n","    エンコーダの順伝播\n","    imgs : 入力画像, [バッチサイズ, チャネル数, 高さ, 幅]\n","    '''\n","    @torch.no_grad()\n","    def forward(self, imgs: torch.Tensor):\n","        # 特徴抽出\n","        features = self.backbone(imgs)\n","\n","        # 並び替え -> [バッチサイズ, 特徴マップの幅 * 高さ, チャネル数]\n","        features = features.permute(0, 2, 3, 1).flatten(1, 2)\n","\n","        return features"]},{"cell_type":"markdown","metadata":{"id":"-R-MJWA8mVwn"},"source":["### アテンション機構の実装"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MsCTNnXGmL59"},"outputs":[],"source":["class Attention(nn.Module):\n","    '''\n","    アテンション機構 (Attention mechanism)\n","    dim_encoder  : エンコーダ出力の特徴次元\n","    dim_decoder  : デコーダ出力の次元\n","    dim_attention: アテンション機構の次元\n","    '''\n","    def __init__(self, dim_encoder: int, \n","                 dim_decoder: int, dim_attention: int):\n","        super().__init__()\n","\n","        # z: エンコーダ出力を変換する線形層(Wz)\n","        self.encoder_att = nn.Linear(dim_encoder, dim_attention)\n","\n","        # h: デコーダ出力を変換する線形層(Wh)\n","        self.decoder_att = nn.Linear(dim_decoder, dim_attention)\n","\n","        # e: アライメントスコアを計算するための線形層\n","        self.full_att = nn.Linear(dim_attention, 1)\n","\n","        # α: アテンション重みを計算する活性化関数\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    '''\n","    Attentionの順伝播\n","    encoder_out   : エンコーダ出力,\n","                    [バッチサイズ, 特徴マップの幅 * 高さ, チャネル数]\n","    decoder_hidden: デコーダ隠れ状態の次元\n","    '''\n","    def forward(self, encoder_out: torch.Tensor, \n","                decoder_hidden: torch.Tensor):\n","        # e: アライメントスコア\n","        att1 = self.encoder_att(encoder_out)    # Wz * z\n","        att2 = self.decoder_att(decoder_hidden) # Wh * h_{t-1}\n","        att = self.full_att(\n","                self.relu(att1 + att2.unsqueeze(1))).squeeze(2) \n","\n","        # α: T個の部分領域ごとのアテンション重み\n","        alpha = att.softmax(dim=1)\n","\n","        # c: コンテキストベクトル\n","        context_vector = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n","\n","        return context_vector, alpha"]},{"cell_type":"markdown","metadata":{"id":"fcfKMzr2mQ3b"},"source":["### アテンション機構付きデコーダの実装"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bJso2qlhmPQ7"},"outputs":[],"source":["class RNNDecoderWithAttention(nn.Module):\n","    '''\n","    アテンション機構付きデコーダネットワーク\n","    dim_attention: アテンション機構の次元\n","    dim_embedding: 埋込み次元\n","    dim_encoder  : エンコーダ出力の特徴量次元\n","    dim_decoder  : デコーダの次元\n","    vocab_size   : 辞書の次元\n","    dropout      : ドロップアウト確率\n","    '''\n","    def __init__(self, dim_attention: int, dim_embedding: int, \n","                 dim_encoder: int, dim_decoder: int,\n","                 vocab_size: int, dropout: float=0.5):\n","        super().__init__()\n","\n","        self.vocab_size = vocab_size\n","\n","        # アテンション機構\n","        self.attention = Attention(dim_encoder, dim_decoder, \n","                                   dim_attention)\n","\n","        # 単語の埋め込み\n","        self.embed = nn.Embedding(vocab_size, dim_embedding)\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # LSTMセル\n","        self.decode_step = nn.LSTMCell(dim_embedding + dim_encoder, \n","                                       dim_decoder, bias=True)\n","\n","        # LSTM隠れ状態/メモリセルの初期値を生成する全結合層\n","        self.init_linear = nn.Linear(dim_encoder, dim_decoder * 2)\n","\n","         # シグモイド活性化前の全結合層\n","        self.f_beta = nn.Linear(dim_decoder, dim_encoder)\n","\n","        # 単語出力用の全結合層\n","        self.linear = nn.Linear(dim_decoder, vocab_size)\n","\n","        # 埋め込み層、全結合層の重みを初期化\n","        self._reset_parameters()\n","        \n","    '''\n","    パラメータの初期化関数\n","    '''\n","    def _reset_parameters(self):\n","        nn.init.uniform_(self.embed.weight, -0.1, 0.1)\n","        nn.init.uniform_(self.linear.weight, -0.1, 0.1)\n","        nn.init.constant_(self.linear.bias, 0)\n","\n","    '''\n","    アテンション機構付きデコーダの順伝播\n","    features: エンコーダ出力,\n","              [バッチサイズ, 特徴マップの幅 * 高さ, チャネル数]\n","    captions: キャプション, [バッチサイズ, 最大系列長]\n","    lengths : 系列長のリスト\n","    '''\n","    def forward(self, features: torch.Tensor, captions: torch.Tensor,\n","                lengths: list):\n","        # バッチサイズの取得\n","        bs = features.shape[0]\n","\n","        # 単語埋込み\n","        embeddings = self.embed(captions)\n","\n","        # 隠れ状態ベクトル、メモリセルの初期値を生成\n","        mean_features = features.mean(dim=1)\n","        init_state = self.init_linear(mean_features)\n","        h, c = init_state.chunk(2, dim=1)\n","\n","        # 最大系列長（<start>を除く）\n","        dec_lengths = [length - 1 for length in lengths]\n","\n","        # キャプショニング結果を保持するためのテンソル\n","        preds = features.new_zeros(\n","            (bs, max(dec_lengths), self.vocab_size))\n","\n","        # センテンス予測処理\n","        for t in range(max(dec_lengths)):\n","            bs_valid = sum([l > t for l in dec_lengths])\n","\n","            # コンテキストベクトル, アテンション重み\n","            context_vector, _ = self.attention(\n","                features[:bs_valid], h[:bs_valid])\n","\n","            # LSTMセル\n","            gate = self.f_beta(h[:bs_valid]).sigmoid()\n","            context_vector = gate * context_vector\n","            context_vector = torch.cat(\n","                (embeddings[:bs_valid, t], context_vector), dim=1)\n","            h, c = self.decode_step(\n","                context_vector, (h[:bs_valid], c[:bs_valid]))\n","            \n","            # 単語予測\n","            pred = self.linear(self.dropout(h))\n","\n","            # 情報保持\n","            preds[:bs_valid, t] = pred\n","\n","        # Show and tellの出力に合わせる\n","        preds = pack_padded_sequence(preds, dec_lengths, \n","                                     batch_first=True)\n","\n","        return preds\n","\n","    '''\n","    サンプリングによる説明文出力（ビームサーチ無し）\n","    features  : エンコーダ出力特徴,\n","                [1, 特徴マップの幅 * 高さ, 埋め込み次元]\n","    word_to_id: 単語->単語ID辞書\n","    '''    \n","    def sample(self, features: torch.Tensor, word_to_id: list):        \n","        # 隠れ状態ベクトル、メモリセルの初期値を生成\n","        mean_features = features.mean(dim=1)\n","        init_state = self.init_linear(mean_features)\n","        h, c = init_state.chunk(2, dim=1)\n","\n","        # センテンス生成の初期値として<start>を埋め込み\n","        id_start = word_to_id['<start>']\n","        prev_word = features.new_tensor((id_start,),\n","                                        dtype=torch.int64)\n","\n","        # サンプリングによるセンテンス生成\n","        preds = []\n","        alphas = []\n","        for _ in range(50):\n","            # 単語埋め込み\n","            embeddings = self.embed(prev_word)\n","\n","            # コンテキストベクトル, アテンション重み\n","            context_vector, alpha = self.attention(features, h)\n","            \n","            # LSTMセル\n","            gate = self.f_beta(h).sigmoid()\n","            context_vector = gate * context_vector\n","            context_vector = torch.cat(\n","                (embeddings, context_vector), dim=1)\n","            h, c = self.decode_step(context_vector, (h, c))\n","\n","            # 単語予測\n","            pred = self.linear(h)\n","            pred = pred.softmax(dim=1)\n","            prev_word = pred.argmax(dim=1)\n","\n","            # 予測結果とアテンション重みを保存\n","            preds.append(prev_word[0].item())\n","            alphas.append(alpha)\n","\n","        return preds, alphas"]},{"cell_type":"markdown","metadata":{"id":"j4x-PO05mCS-"},"source":["###学習におけるハイパーパラメータやオプションの設定"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sJW324Un-mf6"},"outputs":[],"source":["class ConfigTrain(object):\n","    '''\n","    ハイパーパラメータ、グローバル変数の設定\n","    '''  \n","    def __init__(self):\n","\n","        # ハイパーパラメータ\n","        self.enc_img_size = 14     # Attention計算用画像サイズ\n","        self.dim_attention = 128   # Attention層の次元\n","        self.dim_embedding = 128   # 埋め込み層の次元\n","        self.dim_encoder = 2048    # エンコーダの特徴マップのチャネル数\n","        self.dim_hidden = 128      # LSTM隠れ層の次元\n","        self.lr = 0.001            # 学習率\n","        self.dropout = 0.5         # dropout確率\n","        self.batch_size = 30       # ミニバッチ数\n","        self.num_epochs = 30       # エポック数\n","        \n","        # パスの設定\n","        self.img_directory = 'val2014'\n","        self.anno_file = 'drive/MyDrive/python_image_recognition/data/coco2014/captions_val2014.json'\n","        self.word_to_id_file = 'drive/MyDrive/python_image_recognition/6_img_captioning/vocab/word_to_id.pkl'\n","        self.save_directory = 'drive/MyDrive/python_image_recognition/6_img_captioning/model'\n","\n","        # 検証に使う学習セット内のデータの割合\n","        self.val_ratio = 0.3\n","\n","        # データローダーに使うCPUプロセスの数\n","        self.num_workers = 4\n","\n","        # 学習に使うデバイス\n","        self.device = 'cuda'\n","\n","        # 移動平均で計算する損失の値の数\n","        self.moving_avg = 100"]},{"cell_type":"markdown","metadata":{"id":"zbR7QGrr5ouJ"},"source":["### 学習を行う関数"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t1CXRyor-0vl"},"outputs":[],"source":["def train():\n","    config = ConfigTrain()\n","\n","    # 辞書（単語→単語ID）の読み込み\n","    with open(config.word_to_id_file, 'rb') as f:\n","        word_to_id = pickle.load(f)\n","\n","    # 辞書サイズを保存\n","    vocab_size = len(word_to_id)\n","        \n","    # モデル出力用のディレクトリを作成\n","    os.makedirs(config.save_directory, exist_ok=True)\n","\n","    # 画像のtransformsを定義\n","    transforms = T.Compose([\n","        T.Resize((224, 224)),\n","        T.RandomHorizontalFlip(),\n","        T.ToTensor(),\n","        # ImageNetデータセットの平均と標準偏差\n","        T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) \n","    ])\n","\n","    # COCOデータロードの定義\n","    train_dataset = dataset.CocoCaptions(root=config.img_directory, \n","                                         annFile=config.anno_file, \n","                                         transform=transforms)\n","    \n","    # Subset samplerの生成\n","    val_set, train_set = util.generate_subset(\n","        train_dataset, config.val_ratio)\n","\n","    # 学習時にランダムにサンプルするためのサンプラー\n","    train_sampler = SubsetRandomSampler(train_set)\n","\n","    # DataLoaderを生成\n","    collate_func_lambda = lambda x: util.collate_func(x, word_to_id)\n","    train_loader = torch.utils.data.DataLoader(\n","                        train_dataset, \n","                        batch_size=config.batch_size, \n","                        num_workers=config.num_workers, \n","                        sampler=train_sampler,\n","                        collate_fn=collate_func_lambda)\n","    val_loader = torch.utils.data.DataLoader(\n","                        train_dataset, \n","                        batch_size=config.batch_size, \n","                        num_workers=config.num_workers, \n","                        sampler=val_set,\n","                        collate_fn=collate_func_lambda)\n","\n","    # モデルの定義\n","    encoder = CNNEncoder(config.enc_img_size)\n","    decoder = RNNDecoderWithAttention(config.dim_attention,\n","                                      config.dim_embedding, \n","                                      config.dim_encoder,\n","                                      config.dim_hidden,\n","                                      vocab_size,\n","                                      config.dropout)\n","    encoder.to(config.device)\n","    decoder.to(config.device)\n","    \n","    # 損失関数の定義\n","    loss_func = lambda x, y: F.cross_entropy(\n","        x, y, ignore_index=word_to_id.get('<null>', None))\n","    \n","    # 最適化手法の定義\n","    optimizer = torch.optim.AdamW(decoder.parameters(), lr=config.lr)\n","    \n","    # 学習経過の書き込み\n","    now = datetime.datetime.now()\n","    train_loss_file = f'{config.save_directory}/' \\\n","    '6-4_train_loss_{now.strftime(\"%Y%m%d_%H%M%S\")}.csv'\n","    val_loss_file = f'{config.save_directory}/' \\\n","    '6-4_val_loss_{\"%Y%m%d_%H%M%S\"}.csv'\n","\n","    # 学習\n","    val_loss_best = float('inf')\n","    for epoch in range(config.num_epochs):\n","        with tqdm(train_loader) as pbar:\n","            pbar.set_description(f'[エポック {epoch + 1}]')\n","\n","            # 学習モードに設定\n","            encoder.train()\n","            decoder.train()\n","\n","            train_losses = deque()\n","            for i, (imgs, captions, lengths) in enumerate(pbar):\n","                # ミニバッチを設定\n","                imgs = imgs.to(config.device)\n","                captions = captions.to(config.device)\n","\n","                optimizer.zero_grad()\n","\n","                # エンコーダ-デコーダモデル\n","                features = encoder(imgs)\n","                outputs = decoder(features, captions, lengths)\n","\n","                # ロスの計算\n","                captions = captions[:, 1:] \n","                lengths = [length - 1 for length in lengths]\n","                targets = pack_padded_sequence(captions, lengths, \n","                                               batch_first=True)\n","                loss = loss_func(outputs.data, targets.data)\n","\n","                # 誤差逆伝播\n","                loss.backward()\n","                \n","                optimizer.step()\n","\n","                # 学習時の損失をログに書き込み\n","                train_losses.append(loss.item())\n","                if len(train_losses) > config.moving_avg:\n","                    train_losses.popleft()\n","                pbar.set_postfix({\n","                    'loss': torch.Tensor(train_losses).mean().item()})\n","                with open(train_loss_file, 'a') as f:\n","                    print(f'{epoch}, {loss.item()}', file=f)\n","\n","        # 検証\n","        with tqdm(val_loader) as pbar:\n","            pbar.set_description(f'[検証]')\n","\n","            # 評価モード\n","            encoder.eval()\n","            decoder.eval()\n","\n","            val_losses = []\n","            for j, (imgs, captions, lengths) in enumerate(pbar):\n","\n","                # ミニバッチを設定\n","                imgs = imgs.to(config.device)\n","                captions = captions.to(config.device)\n","\n","                # エンコーダ-デコーダモデル\n","                features = encoder(imgs)\n","                outputs = decoder(features, captions, lengths)\n","\n","                # ロスの計算\n","                captions = captions[:, 1:] \n","                lengths = [length - 1 for length in lengths]\n","                targets = pack_padded_sequence(captions, lengths, \n","                                               batch_first=True)\n","                val_loss = loss_func(outputs.data, targets.data)\n","                val_losses.append(val_loss.item())\n","\n","                # Validation Lossをログに書き込み\n","                with open(val_loss_file, 'a') as f:\n","                    print(f'{epoch}, {val_loss.item()}', file=f)\n","\n","        # Loss 表示\n","        val_loss = np.mean(val_losses)\n","        print(f'Validation loss: {val_loss}')\n","\n","        # より良い検証結果が得られた場合、モデルを保存\n","        if val_loss < val_loss_best:\n","            val_loss_best = val_loss\n","\n","            # エンコーダモデルを保存\n","            torch.save(\n","                encoder.state_dict(),\n","                f'{config.save_directory}/6-4_encoder_best.pth')\n","\n","            # デコーダモデルを保存\n","            torch.save(\n","                decoder.state_dict(),\n","                f'{config.save_directory}/6-4_decoder_best.pth')"]},{"cell_type":"markdown","source":["###学習データの解凍"],"metadata":{"id":"3GZUVDi8POUV"}},{"cell_type":"code","source":["!unzip drive/MyDrive/python_image_recognition/data/coco2014/val2014.zip"],"metadata":{"id":"2Edd4dMePPHo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_1VrRcATKFyG"},"source":["###学習の実行"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lKJCJTPoFAok","scrolled":false},"outputs":[],"source":["train()"]},{"cell_type":"markdown","metadata":{"id":"F7vqB5xlE9Z8"},"source":["###デモにおけるハイパーパラメータやオプションの設定"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QOwSV7MwFHKQ"},"outputs":[],"source":["class ConfigDemo(object):\n","    '''\n","    ハイパーパラメータ、グローバル変数の設定\n","    '''  \n","    def __init__(self):\n","\n","        # ハイパーパラメータ\n","        self.enc_img_size = 14     # Attention計算用画像サイズ\n","        self.dim_attention = 128   # Attention層の次元\n","        self.dim_embedding = 128   # 埋め込み層の次元\n","        self.dim_encoder = 2048    # エンコーダの特徴マップのチャネル数\n","        self.dim_hidden = 128      # LSTM隠れ層の次元\n","        \n","        # パスの設定\n","        # 画像キャプショニング推論\n","        self.word_to_id_file = 'drive/MyDrive/python_image_recognition/6_img_captioning/vocab/word_to_id.pkl'\n","        self.id_to_word_file = 'drive/MyDrive/python_image_recognition/6_img_captioning/vocab/id_to_word.pkl'\n","        self.img_dirirectory = 'drive/MyDrive/python_image_recognition/data/image_captioning/'    \n","        self.save_directory = 'drive/MyDrive/python_image_recognition/6_img_captioning/model'\n","        \n","        # 学習に使うデバイス\n","        self.device = 'cuda'"]},{"cell_type":"markdown","metadata":{"id":"GfLOwjeLTxLK"},"source":["###デモを行う関数"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HkP-rZiFFx6t"},"outputs":[],"source":["def demo():\n","    config = ConfigDemo()\n","\n","    # 辞書（単語→単語ID）の読み込み\n","    with open(config.word_to_id_file, 'rb') as f:\n","        word_to_id = pickle.load(f)\n","\n","    # 辞書（単語ID→単語）の読み込み\n","    with open(config.id_to_word_file, 'rb') as f:\n","        id_to_word = pickle.load(f)\n","\n","    # 辞書サイズを保存\n","    vocab_size = len(id_to_word)\n","    \n","    # 画像のtransformsを定義\n","    transforms = T.Compose([\n","        T.Resize((224, 224)),\n","        T.ToTensor(),\n","        # ImageNetデータセットの平均と標準偏差\n","        T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) \n","    ])\n","    \n","    # モデルの定義\n","    encoder = CNNEncoder(config.enc_img_size)\n","    decoder = RNNDecoderWithAttention(config.dim_attention,\n","                                      config.dim_embedding, \n","                                      config.dim_encoder,\n","                                      config.dim_hidden,\n","                                      vocab_size)\n","    encoder.to(config.device)\n","    decoder.to(config.device)\n","    encoder.eval()\n","    decoder.eval()\n","\n","    # モデルの学習済み重みパラメータをロード\n","    encoder.load_state_dict(\n","        torch.load(f'{config.save_directory}/6-4_encoder_best.pth'))\n","    decoder.load_state_dict(\n","        torch.load(f'{config.save_directory}/6-4_decoder_best.pth'))\n","\n","    # ディレクトリ内の画像を対象としてキャプショニング実行\n","    for img_file in sorted(\n","        glob.glob(os.path.join(config.img_dirirectory, '*.jpg'))):\n","\n","        # 画像読み込み\n","        img = Image.open(img_file)\n","        img = transforms(img)\n","        img = img.unsqueeze(0)\n","        img = img.to(config.device)\n","\n","        # エンコーダ・デコーダモデルによる予測\n","        with torch.no_grad():\n","            feature = encoder(img)\n","            sampled_ids, alphas = decoder.sample(feature, word_to_id)\n","\n","        # 入力画像を表示\n","        img_plt = Image.open(img_file)\n","        img_plt = img_plt.resize([224, 224], Image.LANCZOS)\n","        plt.imshow(img_plt)\n","        plt.axis('off')\n","        plt.show()\n","        print(f'入力画像: {os.path.basename(img_file)}')\n","\n","        # 画像キャプショニングの実行\n","        sampled_caption = []\n","        for word_id, alpha in zip(sampled_ids, alphas):\n","            word = id_to_word[word_id]\n","            sampled_caption.append(word)\n","            \n","            alpha = alpha.view(\n","                config.enc_img_size, config.enc_img_size)\n","            alpha = alpha.to('cpu').numpy()\n","            alpha = skimage.transform.pyramid_expand(\n","                alpha, upscale=16, sigma=8)\n","            \n","            # タイムステップtの画像をプロット\n","            plt.imshow(img_plt)\n","            plt.text(0, 1, f'{word}', color='black',\n","                     backgroundcolor='white', fontsize=12)\n","            plt.imshow(alpha, alpha=0.8)\n","            plt.set_cmap(cm.Greys_r)\n","            plt.axis('off')\n","            plt.show()\n","            \n","            if word == '<end>':\n","                break\n","        \n","        sentence = ' '.join(sampled_caption)\n","        print(f'出力キャプション: {sentence}')\n","\n","        # 推定結果を書き込み\n","        gen_sentence_out = img_file[:-4] + '_show_attend_and_tell.txt'\n","        with open(gen_sentence_out, 'w') as f:\n","            print(sentence, file=f)"]},{"cell_type":"markdown","metadata":{"id":"qtLUX-8boUwu"},"source":["###デモの実行"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B9px6r5UUV_b","scrolled":false},"outputs":[],"source":["demo()"]}],"metadata":{"accelerator":"GPU","colab":{"private_outputs":true,"provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"b411586366dd8bdb0fc2800d6aef8fbd6cc14d2a48eec1b457375028a8621915"}}},"nbformat":4,"nbformat_minor":0}