{"cells":[{"cell_type":"markdown","metadata":{"id":"UDnb4-lfp3ya"},"source":["# Pythonで学ぶ画像認識　第６章 画像キャプショニング"]},{"cell_type":"markdown","source":["## 第6.4節 アテンション機構による手法〜Show, attend and tellを実装してみよう"],"metadata":{"id":"tF_B327zbivS"}},{"cell_type":"markdown","source":["### ライブラリの準備"],"metadata":{"id":"oCSbLryKkAUO"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"8mxL-PsjRXaT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671860839644,"user_tz":-540,"elapsed":3657,"user":{"displayName":"Katsuyuki Nakamura","userId":"06073403914299127731"}},"outputId":"debb348f-732b-4617-d020-e713d617b7d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.0+cu116)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.8/dist-packages (2.0.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.23.0)\n","Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from pycocotools) (3.2.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.24.3)\n"]}],"source":["!pip install torch torchvision pycocotools"]},{"cell_type":"markdown","source":["### 実行環境の設定"],"metadata":{"id":"ZMWhlo2oj91m"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15179,"status":"ok","timestamp":1671860855052,"user":{"displayName":"Katsuyuki Nakamura","userId":"06073403914299127731"},"user_tz":-540},"id":"WLEMbaPJOs9v","outputId":"1fabb176-30f0-4ab7-e3fb-f89e8d132592"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!unzip /content/drive/MyDrive/data/coco2014/val2014.zip"],"metadata":{"id":"gMzjavfCKWOr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pmyuus7t-Wwk"},"source":["### モデル学習の前処理（辞書の準備）"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2853,"status":"ok","timestamp":1671860927563,"user":{"displayName":"Katsuyuki Nakamura","userId":"06073403914299127731"},"user_tz":-540},"id":"acj-ALVY-MdY","outputId":"983dd39c-7304-4c22-f0d9-f44995380319"},"outputs":[{"output_type":"stream","name":"stdout","text":["loading annotations into memory...\n","Done (t=0.73s)\n","creating index...\n","index created!\n","単語数: 8583\n"]}],"source":["import pickle\n","from pycocotools.coco import COCO\n","from collections import Counter\n","\n","# データの保存先\n","fp_train_caption = '/content/drive/MyDrive/data/coco2014/captions_val2014.json'\n","fp_word_to_id = '/content/drive/MyDrive/6_image_captioning/vocab/word_to_id.pkl'\n","fp_id_to_word = '/content/drive/MyDrive/6_image_captioning/vocab/id_to_word.pkl'\n","\n","# キャプションを読み込み\n","coco = COCO(fp_train_caption)\n","anns_keys = coco.anns.keys()\n","\n","# 単語ーID対応表の作成\n","coco_token = []\n","for key in anns_keys:\n","    caption = coco.anns[key]['caption']\n","    tokens = caption.lower().split()\n","    coco_token.extend(tokens)\n","\n","# ピリオド、カンマを削除\n","table = str.maketrans({\".\" : \"\",\n","                       \",\" : \"\"})\n","for k in range(len(coco_token)):\n","    coco_token[k] = coco_token[k].translate(table)\n","\n","# 単語ヒストグラムを作成\n","freq = Counter(coco_token)\n","\n","# 3回以上出現する単語を限定して辞書を作成\n","vocab = []\n","common = freq.most_common()\n","for t,c in common:\n","    if c >= 3:\n","        vocab.append(t)\n","sorted(vocab)\n","\n","# 特殊トークンの追加\n","vocab.append('<start>') # 文書の始まりを表すトークンを追加\n","vocab.append('<end>') # 文書の終わりを表すトークンを追加\n","vocab.append('<unk>') # 辞書内に無い単語を表すトークンを追加\n","vocab.append('<null>') # 系列長を揃えるためのトークンを追加\n","\n","# 単語ー単語ID対応表の作成\n","word_to_id = {t:i for i,t in enumerate(vocab)}\n","id_to_word = {i:t for i,t in enumerate(vocab)}\n","\n","# ファイル出力\n","with open(fp_word_to_id, 'wb') as f:\n","    pickle.dump(word_to_id, f)\n","with open(fp_id_to_word, 'wb') as f:\n","    pickle.dump(id_to_word, f)\n","\n","print('単語数: ' + str(len(word_to_id)))"]},{"cell_type":"markdown","metadata":{"id":"yuymkMNhgRWJ"},"source":["### エンコーダの実装"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"zd2Zcbc59rZD","executionInfo":{"status":"ok","timestamp":1671860932299,"user_tz":-540,"elapsed":4741,"user":{"displayName":"Katsuyuki Nakamura","userId":"06073403914299127731"}}},"outputs":[],"source":["import torch\n","from torch import nn\n","from torchvision import models\n","from torch.nn.utils.rnn import pack_padded_sequence\n","\n","# GPUの設定\n","device = 'cuda' if torch.cuda.is_available() else 'cpu' \n","\n","class EncoderCNN(nn.Module):\n","    ''' Show, attend and tellのエンコーダ\n","    encoded_image_size: 画像部分領域サイズ\n","    embedding_dim:      埋込みの次元\n","    '''\n","    def __init__(self, encoded_image_size: int, \n","                 embedding_dim: int):\n","        super(EncoderCNN, self).__init__()\n","        self.enc_image_size = encoded_image_size\n","\n","        # IMAGENET1K_V2で事前学習された\n","        # ResNet152モデルをバックボーンとする\n","        resnet = models.resnet152(weights=\"IMAGENET1K_V2\") \n","        \n","        # プーリング層と全結合層を削除\n","        modules = list(resnet.children())[:-2]\n","        self.resnet = nn.Sequential(*modules)\n","\n","        # AdaptiveAvgPool2dで部分領域(14x14)を作成\n","        self.adaptive_pool = nn.AdaptiveAvgPool2d(\n","                                (encoded_image_size, \n","                                 encoded_image_size))\n","\n","    ''' エンコーダの順伝播\n","    images : 入力画像テンソル [バッチサイズ, チャネル数, 高さ, 幅]\n","    '''\n","    def forward(self, images: torch.Tensor):\n","        # 特徴抽出\n","        features = self.resnet(images) \n","        features = self.adaptive_pool(features)\n","\n","        # 並び替え -> [バッチサイズ, 14, 14, 2048]\n","        features = features.permute(0, 2, 3, 1)\n","\n","        return features"]},{"cell_type":"markdown","source":["### アテンション機構の実装"],"metadata":{"id":"-R-MJWA8mVwn"}},{"cell_type":"code","source":["class Attention(nn.Module):\n","    ''' アテンション機構 (Attention mechanism)\n","    encoder_dim: エンコーダ出力の特徴次元\n","    decoder_dim: デコーダ出力の次元\n","    attention_dim: アテンション機構の次元\n","    '''\n","    def __init__(self, encoder_dim: int, \n","                 decoder_dim: int, attention_dim: int):\n","        super(Attention, self).__init__()\n","\n","        # z: エンコーダ出力を変換する線形層(Wz)\n","        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n","\n","        # h: デコーダ出力を変換する線形層(Wh)\n","        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n","\n","        # e: アライメントスコアを計算するための線形層\n","        self.full_att = nn.Linear(attention_dim, 1)\n","\n","        # α: アテンション重みを計算する活性化関数/ソフトマックス層\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    ''' Attentionの順伝播\n","    encoder_out: エンコーダ出力\n","    decoder_hidden: デコーダ隠れ状態の次元\n","    '''\n","    def forward(self, encoder_out: torch.Tensor, \n","                decoder_hidden: torch.Tensor):\n","\n","        # e: アライメントスコア\n","        att1 = self.encoder_att(encoder_out) # Wz * z\n","        att2 = self.decoder_att(decoder_hidden) # Wh * h_{t-1}\n","        att = self.full_att(\n","                self.relu(att1 + att2.unsqueeze(1))).squeeze(2) \n","\n","        # α: T個の部分領域ごとのアテンション重み\n","        alpha = self.softmax(att)\n","\n","        # c: コンテキストベクトル\n","        context_vector = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n","\n","        return context_vector, alpha"],"metadata":{"id":"MsCTNnXGmL59","executionInfo":{"status":"ok","timestamp":1671860932300,"user_tz":-540,"elapsed":20,"user":{"displayName":"Katsuyuki Nakamura","userId":"06073403914299127731"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### アテンション機構付きデコーダの実装"],"metadata":{"id":"fcfKMzr2mQ3b"}},{"cell_type":"code","source":["class DecoderWithAttention(nn.Module):\n","    ''' アテンション機構 (Attention mechanism)付きデコーダネットワーク\n","    attention_dim: アテンション機構の次元\n","    embed_dim: 埋込み次元\n","    decoder_dim: デコーダの次元\n","    vocab_size: 辞書の次元\n","    encoder_dim: エンコーダ出力の特徴次元\n","    '''\n","    def __init__(self, attention_dim: int, embed_dim: int, \n","                 decoder_dim: int, vocab_size: int, \n","                 encoder_dim: int=2048, dropout: float=0.5):\n","        super(DecoderWithAttention, self).__init__()\n","\n","        # パラメータ\n","        self.encoder_dim = encoder_dim\n","        self.attention_dim = attention_dim\n","        self.embed_dim = embed_dim\n","        self.decoder_dim = decoder_dim\n","        self.vocab_size = vocab_size\n","        self.dropout = dropout\n","\n","        # アテンション機構\n","        self.attention = Attention( encoder_dim, \n","                                    decoder_dim, \n","                                    attention_dim)\n","\n","        # 単語の埋め込み\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","        self.dropout = nn.Dropout(p=self.dropout)\n","\n","        # LSTMセル\n","        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, \n","                                       decoder_dim, bias=True)\n","\n","        # LSTM隠れ状態/メモリセルを初期化\n","        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n","        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n","\n","         # シグモイド活性化前の線形層\n","        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n","        self.sigmoid = nn.Sigmoid()\n","\n","        # 単語出力用の線形層\n","        self.fc = nn.Linear(decoder_dim, vocab_size)\n","\n","        # 埋め込み層、全結合層の重みを初期化\n","        self.init_weights()\n","        \n","    '''\n","    デコーダの重みパラメータを初期化\n","    '''\n","    def init_weights(self):\n","        self.embedding.weight.data.uniform_(-0.1, 0.1)\n","        self.fc.bias.data.fill_(0)\n","        self.fc.weight.data.uniform_(-0.1, 0.1)\n","\n","    ''' 画像特徴の平均値で隠れ状態とメモリセルを初期化\n","    encoder_out: エンコーダ出力 [バッチサイズ, 14, 14, 2048]\n","    '''\n","    def init_hidden_state(self, encoder_out: torch.Tensor):\n","        mean_encoder_out = encoder_out.mean(dim=1)\n","        h = self.init_h(mean_encoder_out)\n","        c = self.init_c(mean_encoder_out)\n","        return h, c\n","\n","    ''' アテンション機構付きデコーダの順伝播\n","    encoder_out: エンコーダ出力 [バッチサイズ, 14, 14, 2048]\n","    encoded_captions: キャプション [バッチサイズ, 最大系列長]\n","    caption_lengths: 系列長 [バッチサイズ, 1]\n","    '''\n","    def forward(self, encoder_out: torch.Tensor, \n","                encoded_captions: torch.Tensor,\n","                caption_lengths: list):\n","        # パラメータ\n","        batch_size = encoder_out.size(0)\n","        encoder_dim = encoder_out.size(-1)\n","        vocab_size = self.vocab_size\n","\n","        # エンコーダ出力特徴の平坦化\n","        encoder_out = encoder_out.view(batch_size, -1, encoder_dim) \n","        num_pixels = encoder_out.size(1)\n","\n","        # 単語埋込み\n","        embedded_captions = self.embedding(encoded_captions)\n","\n","        # 隠れ状態ベクトル、メモリセルを初期化\n","        h, c = self.init_hidden_state(encoder_out)\n","\n","        # 最大系列長（<end>を除く）\n","        caption_lengths = torch.tensor(caption_lengths)\n","        dec_lengths = (caption_lengths - 1).tolist()\n","\n","        # キャプショニング結果を保持するためのテンソル\n","        predictions = torch.zeros(batch_size, \n","                                    max(dec_lengths), \n","                                    vocab_size).to(device)\n","\n","        # アテンション重みを保持するためのテンソル\n","        alphas = torch.zeros(batch_size, \n","                                max(dec_lengths), \n","                                num_pixels).to(device)\n","\n","        # センテンス予測処理\n","        for t in range(max(dec_lengths)):\n","            batch_size_t = sum([l > t for l in dec_lengths])\n","\n","            # コンテキストベクトル, アテンション重み\n","            context_vector, alpha = self.attention(\n","                                        encoder_out[:batch_size_t],\n","                                        h[:batch_size_t])\n","\n","            # LSTMセル\n","            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))\n","            context_vector = gate * context_vector\n","            h, c = self.decode_step(\n","                torch.cat([embedded_captions[:batch_size_t, t, :],\n","                            context_vector], dim=1),\n","                (h[:batch_size_t], c[:batch_size_t]))\n","            preds = self.fc(self.dropout(h))\n","\n","            # 情報保持\n","            predictions[:batch_size_t, t, :] = preds\n","            alphas[:batch_size_t, t, :] = alpha\n","\n","        # Show and tellの出力に合わせる\n","        encoded_captions = encoded_captions[:, 1:] \n","        predictions = pack_padded_sequence(predictions, \n","                                            dec_lengths, \n","                                            batch_first=True)\n","        encoded_captions = pack_padded_sequence(encoded_captions, \n","                                                dec_lengths, \n","                                                batch_first=True)\n","\n","        return predictions.data, encoded_captions.data, \\\n","               dec_lengths, alphas\n","\n","    ''' サンプリングによる説明文出力（ビームサーチ無し）\n","    features:   エンコーダ出力特徴 [バッチサイズ, 埋め込み次元]\n","    word_to_id: 単語->単語ID辞書\n","    id_to_word: 単語ID->単語辞書\n","    '''    \n","    def sample(self, feature: torch.Tensor, \n","               word_to_id: list, id_to_word: list, \n","               states=None):\n","        vocab_size = self.vocab_size\n","\n","        # エンコーダ出力特徴の平坦化\n","        enc_image_size = feature.size(1)\n","        encoder_dim = feature.size(-1)\n","        feature = feature.view(1, -1, encoder_dim)\n","        num_pixels = feature.size(1)\n","        feature = feature.expand(1, num_pixels, encoder_dim)\n","        \n","        # 隠れ状態ベクトル、メモリセルを初期化\n","        h, c = self.init_hidden_state(feature)\n","\n","        # センテンス生成の初期値として<start>を埋め込み\n","        id_start = word_to_id['<start>']\n","        prev_words = torch.LongTensor([[id_start]]).to(device) \n","\n","        # サンプリングによるセンテンス生成\n","        predictions = []\n","        alphas = []\n","        step = 1\n","        while True:\n","            # 単語埋め込み\n","            embedded_captions = self.embedding(prev_words).squeeze(1)\n","\n","            # アテンション重み/コンテキストベクトルの計算\n","            context_vector, alpha = self.attention(feature,h)\n","            alpha = alpha.view(-1, enc_image_size, enc_image_size)  \n","            gate = self.sigmoid(self.f_beta(h))\n","            context_vector = gate * context_vector\n","\n","            # デコード処理\n","            h, c = self.decode_step(\n","                torch.cat([embedded_captions, context_vector], \n","                            dim=1), (h, c))\n","\n","            preds = self.fc(self.dropout(h))\n","            preds = torch.nn.functional.log_softmax(preds)\n","            \n","            # 単語予測\n","            prob, predicted = preds.max(1)\n","            word = id_to_word[predicted.item()]\n","\n","            # 予測結果とアテンション重みを保存\n","            predictions.append(predicted)\n","            alphas.append(alpha)\n","\n","            # 次のタイムステップへ\n","            prev_words = torch.LongTensor(\n","                [predicted.item()]).to(device) \n","\n","            # 系列が長くなりすぎたらBreak\n","            if step > 50:\n","                break\n","            step += 1\n","\n","        return predictions, alphas"],"metadata":{"id":"bJso2qlhmPQ7","executionInfo":{"status":"ok","timestamp":1671867227177,"user_tz":-540,"elapsed":418,"user":{"displayName":"Katsuyuki Nakamura","userId":"06073403914299127731"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NO9mjvMUmdUt"},"source":["### データローダの実装"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"c8j_wqIA-hT5","executionInfo":{"status":"ok","timestamp":1671860932301,"user_tz":-540,"elapsed":20,"user":{"displayName":"Katsuyuki Nakamura","userId":"06073403914299127731"}}},"outputs":[],"source":["import pickle\n","import random\n","import torch\n","import torchvision.transforms as transforms\n","import torchvision.datasets as dset\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from pycocotools.coco import COCO\n","\n","'''\n","COCOデータセットローダ\n","batch_size:         バッチサイズ\n","word_to_id:         単語->単語ID辞書\n","fp_train_caption:   学習用のキャプション\n","fp_train_image_dir: 学習画像のパス\n","'''\n","def COCO_loader(batch_size: int, word_to_id: list, \n","                fp_train_caption: str, \n","                fp_train_image_dir: str):\n","\n","    ''' トークナイザ\n","    文章(caption)を単語IDのリスト(tokens_id)に変換\n","    caption: 画像キャプション [バッチサイズ, 系列長]\n","    '''\n","    def tokenize_caption(caption: torch.Tensor):\n","        # 単語についたピリオド、カンマを削除\n","        tokens = caption.lower().split()\n","        tokens_temp = []\n","        for t in tokens:\n","            if t.endswith('.') and t != '.':\n","                tokens_temp.append(t.replace('.', ''))\n","            elif t.endswith(',') and t != ',':\n","                tokens_temp.append(t.replace(',', ''))\n","            elif t == '.' or t == ',':\n","                continue\n","            else:\n","                tokens_temp.append(t)\n","        tokens = tokens_temp        \n","        \n","        # 文章(caption)を単語IDのリスト(tokens_id)に変換\n","        tokens_ext = ['<start>'] + tokens + ['<end>']\n","        tokens_id = []\n","        for k in tokens_ext:\n","            if k in word_to_id:\n","                tokens_id.append(word_to_id[k])\n","            else:\n","                tokens_id.append(word_to_id['<unk>'])\n","        return torch.Tensor(tokens_id)\n","\n","    '''\n","    COCOデータセットからデータを取り出すためのcollate関数\n","    '''\n","    def cap_collate_fn(data):\n","        images, captions = zip(*data)\n","        captions = [tokenize_caption(cap[random.randrange(len(cap))]) for cap in captions]\n","        \n","        data = zip(images, captions)\n","        data = sorted(data, key=lambda x: len(x[1]), reverse=True)\n","        images, captions = zip(*data)\n","        images = torch.stack(images, 0)\n","\n","        lengths = [len(c) for c in captions]\n","        targets = torch.zeros(len(captions), max(lengths)).long()\n","        targets[:] = word_to_id['<null>']   # nullでパディング\n","        for i,c in enumerate(captions):\n","            end = lengths[i]\n","            targets[i,:end] = c[:end]\n","        return images, targets, lengths\n"," \n","    # 画像のtransformsを定義\n","    crop_size = (224,224)             # CNN入力画像サイズ\n","    in_mean = (0.485, 0.456, 0.406)   # ImageNetの平均値\n","    in_std = (0.229, 0.224, 0.225)    # ImageNetの標準偏差\n","    trans = transforms.Compose([\n","            transforms.Resize(crop_size),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","            transforms.Normalize(in_mean, in_std) \n","    ])\n","\n","    # COCOデータロードの定義\n","    train_val_set = dset.CocoCaptions(root=fp_train_image_dir, \n","                                        annFile=fp_train_caption, \n","                                        transform=trans)\n","            \n","    # データサブセットを取得するサンプラーの定義\n","    # 学習データ70%、評価データ30%に分割\n","    n_samples = len(train_val_set)\n","    indices = list(range(n_samples))\n","    tr_split = int(0.7 * n_samples)      \n","    train_idx, val_idx = indices[:tr_split], indices[tr_split:]\n","    train_sampler = SubsetRandomSampler(train_idx)\n","    val_sampler = SubsetRandomSampler(val_idx)\n","\n","    # Dataloaderを生成\n","    train_loader = torch.utils.data.DataLoader(\n","                        train_val_set, \n","                        batch_size=batch_size, \n","                        num_workers=4, \n","                        sampler=train_sampler,\n","                        collate_fn=cap_collate_fn)\n","\n","    val_loader = torch.utils.data.DataLoader(\n","                        train_val_set, \n","                        batch_size=batch_size, \n","                        num_workers=4, \n","                        sampler=val_sampler,\n","                        collate_fn=cap_collate_fn)\n","                                            \n","    return train_loader, val_loader"]},{"cell_type":"markdown","metadata":{"id":"j4x-PO05mCS-"},"source":["### Configクラスの実装"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"sJW324Un-mf6","executionInfo":{"status":"ok","timestamp":1671860932301,"user_tz":-540,"elapsed":19,"user":{"displayName":"Katsuyuki Nakamura","userId":"06073403914299127731"}}},"outputs":[],"source":["import os\n","import pickle\n","\n","class Config(object):\n","    '''\n","    ハイパーパラメータ、グローバル変数の設定\n","    '''    \n","    def __init__(self):\n","\n","        # ハイパーパラメータ（Show, attend and tell用）\n","        self.enc_image_size = 14    # Attention計算用画像サイズ\n","        self.attention_dim = 128    # Attention層の次元\n","        self.embedding_dim = 128    # 埋め込み層の次元\n","        self.hidden_dim = 128       # LSTM隠れ層の次元\n","        self.num_layers = 2         # LSTM階層の数\n","        self.max_seg_len = 30       # 最大系列長\n","\n","        # ハイパーパラメータ（学習用）\n","        self.learning_rate = 0.001  # 学習率\n","        self.batch_size = 30        # ミニバッチの数\n","        self.num_epochs = 30        # エポック\n","        \n","        # グローバル変数\n","        self.fp_train_cap = '/content/drive/MyDrive/data/coco2014/captions_val2014.json'\n","        self.fp_train_image_dir = 'val2014'\n","        self.fp_word_to_id = '/content/drive/MyDrive/6_image_captioning/vocab/word_to_id.pkl'\n","        self.fp_id_to_word = '/content/drive/MyDrive/6_image_captioning/vocab/id_to_word.pkl'\n","        self.fp_model_dir = '/content/drive/MyDrive/6_image_captioning/model'\n","\n","        # 辞書（単語→単語ID）の読み込み\n","        with open(self.fp_word_to_id, 'rb') as f:\n","            self.word_to_id = pickle.load(f)\n","\n","        # 辞書（単語ID→単語）の読み込み\n","        with open(self.fp_id_to_word, 'rb') as f:\n","            self.id_to_word = pickle.load(f)\n","\n","        # 辞書サイズ\n","        self.vocab_size = len(self.word_to_id)\n","\n","        # モデル出力用のディレクトリ\n","        if not(os.path.isdir(self.fp_model_dir)):\n","            os.makedirs(self.fp_model_dir)"]},{"cell_type":"markdown","metadata":{"id":"zbR7QGrr5ouJ"},"source":["### 学習実装"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["dc77b015b6cf4d709de4d623fb35ceee","a6802706513241b2a0535ff81d8b7806","d5d8583dffd54565a533d10f48982ff9","c804627e0fb4487180d44e05d1f0b173","65217be064ad49e6ab86f77eb90f4b54","e470c30f09754fd487031f7642d45713","6a48d8f78ea0463d96a69cdfef62d35f","1b072bd725c64e0e82308e8d306a0be6","2f31b70d35f4464b99f82e23229552ce","4301d22fe53b426e858fd743d828793e","2227bd076f454cbbb785905234bdb4f9"]},"executionInfo":{"elapsed":6037497,"status":"ok","timestamp":1671866969779,"user":{"displayName":"Katsuyuki Nakamura","userId":"06073403914299127731"},"user_tz":-540},"id":"t1CXRyor-0vl","outputId":"cd8de712-fe4e-4435-efe3-47e19b7ab582"},"outputs":[{"output_type":"stream","name":"stdout","text":["loading annotations into memory...\n","Done (t=0.34s)\n","creating index...\n","index created!\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet152-f82ba261.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-f82ba261.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/230M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc77b015b6cf4d709de4d623fb35ceee"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["学習開始\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 1]: 100%|██████████| 946/946 [03:04<00:00,  5.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 4.6005125449021325\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 1]: 100%|██████████| 406/406 [00:31<00:00, 12.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 3.8196538105386817\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 2]: 100%|██████████| 946/946 [02:45<00:00,  5.71it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 3.765526287399437\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 2]: 100%|██████████| 406/406 [00:32<00:00, 12.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 3.4387045576067394\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 3]: 100%|██████████| 946/946 [02:46<00:00,  5.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 3.529830376643207\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 3]: 100%|██████████| 406/406 [00:32<00:00, 12.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 3.2687661371794827\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 4]: 100%|██████████| 946/946 [02:46<00:00,  5.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 3.3930501428769455\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 4]: 100%|██████████| 406/406 [00:32<00:00, 12.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 3.159297685317805\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 5]: 100%|██████████| 946/946 [02:48<00:00,  5.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 3.3002687049970567\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 5]: 100%|██████████| 406/406 [00:32<00:00, 12.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 3.087300557808336\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 6]: 100%|██████████| 946/946 [02:46<00:00,  5.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 3.217187118328651\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 6]: 100%|██████████| 406/406 [00:32<00:00, 12.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 3.0351133023576784\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 7]: 100%|██████████| 946/946 [02:47<00:00,  5.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 3.1607824538777294\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 7]: 100%|██████████| 406/406 [00:32<00:00, 12.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 3.0016791033627364\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 8]: 100%|██████████| 946/946 [02:47<00:00,  5.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 3.126956529395525\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 8]: 100%|██████████| 406/406 [00:32<00:00, 12.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 2.953347954844019\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 9]: 100%|██████████| 946/946 [02:46<00:00,  5.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 3.0781797735181975\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 9]: 100%|██████████| 406/406 [00:32<00:00, 12.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 2.92090480433309\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 10]: 100%|██████████| 946/946 [02:47<00:00,  5.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 3.052268370261404\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 10]: 100%|██████████| 406/406 [00:32<00:00, 12.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 2.906334676178805\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 11]: 100%|██████████| 946/946 [02:46<00:00,  5.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 3.0153299623009517\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 11]: 100%|██████████| 406/406 [00:32<00:00, 12.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 2.884445573896023\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 12]: 100%|██████████| 946/946 [02:46<00:00,  5.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 2.988496896328432\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 12]: 100%|██████████| 406/406 [00:32<00:00, 12.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 2.880455274887273\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 13]: 100%|██████████| 946/946 [02:47<00:00,  5.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 2.963229777444997\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 13]: 100%|██████████| 406/406 [00:32<00:00, 12.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 2.8317553668186584\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 14]: 100%|██████████| 946/946 [02:47<00:00,  5.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 2.940013558617606\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 14]: 100%|██████████| 406/406 [00:32<00:00, 12.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 2.8208623317074895\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 15]: 100%|██████████| 946/946 [02:46<00:00,  5.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 2.9227093281755994\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 15]: 100%|██████████| 406/406 [00:32<00:00, 12.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 2.8147060066608374\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 16]: 100%|██████████| 946/946 [02:47<00:00,  5.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 2.9089824850917116\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 16]: 100%|██████████| 406/406 [00:32<00:00, 12.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 2.8100951587038088\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 17]: 100%|██████████| 946/946 [02:47<00:00,  5.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 2.8810201376739566\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 17]: 100%|██████████| 406/406 [00:32<00:00, 12.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 2.795070484353991\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 18]: 100%|██████████| 946/946 [02:47<00:00,  5.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 2.8766321289110888\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 18]: 100%|██████████| 406/406 [00:32<00:00, 12.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 2.793437493845747\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 19]: 100%|██████████| 946/946 [02:47<00:00,  5.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 2.85223215377356\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 19]: 100%|██████████| 406/406 [00:32<00:00, 12.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 2.782191715804227\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 20]: 100%|██████████| 946/946 [02:47<00:00,  5.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 2.8419237250253464\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 20]: 100%|██████████| 406/406 [00:32<00:00, 12.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 2.7720210951537334\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 21]: 100%|██████████| 946/946 [02:46<00:00,  5.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 2.829322222171324\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 21]: 100%|██████████| 406/406 [00:32<00:00, 12.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 2.7596496971957203\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 22]: 100%|██████████| 946/946 [02:47<00:00,  5.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 2.8213256529471336\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 22]: 100%|██████████| 406/406 [00:32<00:00, 12.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 2.768274192152352\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 23]: 100%|██████████| 946/946 [02:47<00:00,  5.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 2.8091993221016818\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 23]: 100%|██████████| 406/406 [00:32<00:00, 12.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 2.74632114406877\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 24]: 100%|██████████| 946/946 [02:47<00:00,  5.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 2.7911395388980242\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 24]: 100%|██████████| 406/406 [00:32<00:00, 12.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 2.750002176009963\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 25]: 100%|██████████| 946/946 [02:47<00:00,  5.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 2.78305109509958\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 25]: 100%|██████████| 406/406 [00:32<00:00, 12.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 2.751508169573516\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 26]: 100%|██████████| 946/946 [02:46<00:00,  5.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 2.78392867457287\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 26]: 100%|██████████| 406/406 [00:32<00:00, 12.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 2.734361811518082\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 27]: 100%|██████████| 946/946 [02:47<00:00,  5.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 2.7758394215122086\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 27]: 100%|██████████| 406/406 [00:32<00:00, 12.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 2.735458015808331\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 28]: 100%|██████████| 946/946 [02:46<00:00,  5.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 2.7574122375464087\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 28]: 100%|██████████| 406/406 [00:32<00:00, 12.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 2.7402892500308935\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 29]: 100%|██████████| 946/946 [02:47<00:00,  5.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 2.7499215683523746\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 29]: 100%|██████████| 406/406 [00:32<00:00, 12.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 2.7366784576124745\n"]},{"output_type":"stream","name":"stderr","text":["[Train epoch 30]: 100%|██████████| 946/946 [02:47<00:00,  5.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training loss: 2.7475697679197055\n"]},{"output_type":"stream","name":"stderr","text":["[Validation 30]: 100%|██████████| 406/406 [00:32<00:00, 12.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 2.731278054232668\n","学習終了\n"]}],"source":["import os\n","import pickle\n","import numpy as np\n","import datetime\n","import torch\n","from torch import nn\n","from torch.nn.utils.rnn import pack_padded_sequence\n","from tqdm import tqdm\n","from torch.utils.data.dataset import Subset\n","\n","'''\n","Show, attend and Tellの学習\n","'''\n","def train():\n","\n","    # GPUの設定\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n","\n","    # ハイパーパラメータの設定\n","    cfg = Config()\n","\n","    # 学習データの読み込み\n","    train_loader, valid_loader = COCO_loader(cfg.batch_size, \n","                                    cfg.word_to_id, \n","                                    cfg.fp_train_cap, \n","                                    cfg.fp_train_image_dir)\n","    # モデルの定義\n","    encoder = EncoderCNN(cfg.enc_image_size, \n","                         cfg.embedding_dim).to(device)\n","    decoder = DecoderWithAttention(cfg.attention_dim, \n","                                    cfg.embedding_dim, \n","                                    cfg.hidden_dim, \n","                                    cfg.vocab_size).to(device)\n","    \n","    # 損失関数の定義\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # 最適化手法の定義\n","    params = list(decoder.parameters()) + \\\n","             list(encoder.adaptive_pool.parameters())\n","    optimizer = torch.optim.AdamW(params, lr=cfg.learning_rate)\n","\n","    # 学習経過の書き込み\n","    now = datetime.datetime.now()\n","    fp_train_loss_out = '{}/6-4_train_loss_{}.csv'\\\n","        .format(cfg.fp_model_dir, now.strftime('%Y%m%d_%H%M%S'))\n","    fp_val_loss_out = '{}/6-4_val_loss_{}.csv'\\\n","        .format(cfg.fp_model_dir, now.strftime('%Y%m%d_%H%M%S'))\n","\n","    # 学習\n","    print(\"学習開始\")\n","    val_loss_best = float('inf')\n","    for epoch in range(cfg.num_epochs):\n","        with tqdm(train_loader) as pbar:\n","            pbar.set_description(\"[Train epoch %d]\" % (epoch + 1))\n","            train_losses = []\n","            for i, (images, captions,lengths) in enumerate(pbar):\n","\n","                # 学習モード\n","                encoder.train()\n","                decoder.train()\n","\n","                # ミニバッチを設定\n","                images, captions = \\\n","                    images.to(device), captions.to(device)\n","                targets = pack_padded_sequence(captions, \n","                                               lengths, \n","                                               batch_first=True)[0]                \n","                optimizer.zero_grad()\n","\n","                # Forward\n","                features = encoder(images)\n","                outputs, targets, decode_lengths, alphas = \\\n","                    decoder(features, captions, lengths)\n","                loss = criterion(outputs, targets)\n","\n","                # backward\n","                loss.backward()\n","                optimizer.step()\n","\n","                # Training Lossをログに書き込み\n","                train_losses.append(loss.item())\n","                with open(fp_train_loss_out, 'a') as f:\n","                    print(\"{},{}\".format(epoch, loss.item()), file=f)\n","\n","        # Loss 表示\n","        print(\"Training loss: {}\".format(np.average(train_losses)))\n","\n","        # validation\n","        with tqdm(valid_loader) as pbar:\n","            pbar.set_description(\"[Validation %d]\" % (epoch + 1))\n","            val_losses = []\n","            for j, (images, captions,lengths) in enumerate(pbar):\n","\n","                # 評価モード\n","                encoder.eval()\n","                decoder.eval()\n","\n","                # ミニバッチを設定\n","                images, captions = \\\n","                    images.to(device), captions.to(device)\n","                targets = pack_padded_sequence(captions, \n","                                               lengths, \n","                                               batch_first=True)[0]\n","\n","                features = encoder(images)\n","                outputs, targets, decode_lengths, alphas = \\\n","                    decoder(features, captions, lengths)\n","                val_loss = criterion(outputs, targets)\n","                val_losses.append(val_loss.item())\n","\n","                # Validation Lossをログに書き込み\n","                with open(fp_val_loss_out, 'a') as f:\n","                    print(\"{},{}\".format(epoch, val_loss.item()), file=f)\n","\n","        # Loss 表示\n","        val_loss = np.average(val_losses)\n","        print(\"Validation loss: {}\".format(val_loss))\n","\n","        # より良い検証結果が得られた場合、モデルを保存\n","        if val_loss < val_loss_best:\n","            val_loss_best = val_loss\n","\n","            # エンコーダモデルを保存\n","            fp_encoder = '{}/6-4_encoder_best.pth'.format(cfg.fp_model_dir)\n","            torch.save(encoder.to('cpu').state_dict(), fp_encoder)\n","            encoder.to(device)\n","\n","            # デコーダモデルを保存\n","            fp_decoder = '{}/6-4_decoder_best.pth'.format(cfg.fp_model_dir)\n","            torch.save(decoder.to('cpu').state_dict(), fp_decoder)\n","            decoder.to(device)\n","    \n","    print(\"学習終了\")\n","\n","if __name__ == '__main__':\n","    train()"]},{"cell_type":"markdown","metadata":{"id":"GfLOwjeLTxLK"},"source":["### 推論（画像キャプショニング）"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"YMnhZIPST4et","executionInfo":{"status":"ok","timestamp":1671867356567,"user_tz":-540,"elapsed":351,"user":{"displayName":"Katsuyuki Nakamura","userId":"06073403914299127731"}}},"outputs":[],"source":["import torch\n","import torchvision.transforms as transforms\n","import glob\n","import os\n","import matplotlib.pyplot as plt\n","import skimage.transform\n","import matplotlib.cm as cm\n","from PIL import Image\n","\n","''' 画像読み込み\n","image_file:   画像ファイル\n","transform:    画像変換\n","'''\n","def load_image(image_file: str, transform=None):\n","    image = Image.open(image_file)\n","    image = image.resize([224, 224], Image.LANCZOS)\n","    if transform is not None:\n","        image = transform(image).unsqueeze(0)\n","    return image\n","\n","''' \n","画像キャプショニングの推論\n","'''\n","def infer(fp_encoder: str, fp_decoder: str, fp_infer_image_dir: str):\n","  \n","    # GPUを利用\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    print(\"Running in %s.\" % device)\n","\n","    # パラメータ設定\n","    cfg = Config()\n","    \n","    # 画像の正規化\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","    ])\n","\n","    # エンコーダモデルの定義\n","    encoder = EncoderCNN(cfg.enc_image_size, cfg.embedding_dim)\n","    encoder = encoder.to(device).eval()\n","\n","    # デコーダモデルの定義\n","    decoder = DecoderWithAttention(cfg.attention_dim, cfg.embedding_dim, cfg.hidden_dim, cfg.vocab_size)\n","    decoder = decoder.to(device).eval()\n","\n","    # モデルの学習済み重みパラメータをロード\n","    encoder.load_state_dict(torch.load(fp_encoder), strict=False)\n","    decoder.load_state_dict(torch.load(fp_decoder), strict=False)\n","    print('エンコーダ: {}'.format(fp_encoder))\n","    print('デコーダ: {}'.format(fp_decoder))\n","\n","    for image_file in sorted(glob.glob(os.path.join(fp_infer_image_dir, \"*.jpg\"))):\n","\n","        # 画像読み込み\n","        print(\"ファイル名: {}\".format(os.path.basename(image_file)))\n","        image = load_image(image_file, transform).to(device)\n","\n","        # Encoder-decoderによる予測\n","        with torch.no_grad():\n","\n","            # encoder\n","            feature = encoder(image)\n","            enc_image_size = feature.size(1)\n","            encoder_dim = feature.size(3)\n","\n","            # decoder\n","            predictions, alphas = decoder.sample(feature, cfg.word_to_id, cfg.id_to_word)\n","\n","        # 可視化\n","        sampled_caption = []\n","        word_len = len(predictions)\n","        image_plt = Image.open(image_file)\n","        image_plt = image_plt.resize([224, 224], Image.LANCZOS)\n","        plt.imshow(image_plt)\n","        plt.axis('off')\n","        plt.show()\n","        for t in range(word_len):\n","\n","            # Attention重みを可視化\n","            cur_alpha = alphas[t]\n","            alpha = cur_alpha.to('cpu').numpy()\n","            alpha = skimage.transform.pyramid_expand(alpha[0, :, :], upscale=16, sigma=8)\n","\n","            # キャプショニング\n","            word_id = predictions[t]\n","            word = cfg.id_to_word[word_id.item()]\n","            sampled_caption.append(word)\n","\n","            # タイムステップtの画像をプロット\n","            plt.imshow(image_plt)\n","            plt.text(0, 1, '%s' % (word), color='black', backgroundcolor='white', fontsize=12)\n","            plt.imshow(alpha, alpha=0.8)\n","            plt.set_cmap(cm.Greys_r)\n","            plt.axis('off')\n","            plt.show()\n","\n","            if word == '<end>':\n","                break\n","        \n","        sentence = ' '.join(sampled_caption)\n","        print (\"  {}\".format(sentence))\n","\n","        # 推定結果を書き込み\n","        gen_sentence_out = image_file[:-4] + \"_show_attend_and_tell.txt\"\n","        with open(gen_sentence_out, 'w') as f:\n","            print(\"{}\".format(sentence), file=f)"]},{"cell_type":"markdown","source":["### 推論の実行"],"metadata":{"id":"qtLUX-8boUwu"}},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1hpWoL_ydEE-XMoLTdmKSEVT6cp9Pj9Y9"},"executionInfo":{"elapsed":19759,"status":"ok","timestamp":1671867380695,"user":{"displayName":"Katsuyuki Nakamura","userId":"06073403914299127731"},"user_tz":-540},"id":"B9px6r5UUV_b","outputId":"8566b748-dbf9-4aad-b69e-3f3b447c5718"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# 画像キャプショニング推論\n","fp_encoder = '/content/drive/MyDrive/6_image_captioning/model/6-4_encoder_best.pth'\n","fp_decoder = '/content/drive/MyDrive/6_image_captioning/model/6-4_decoder_best.pth'\n","fp_infer_image_dir = '/content/drive/MyDrive/data/image_captioning/'    \n","\n","infer(fp_encoder, fp_decoder, fp_infer_image_dir)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"toc_visible":true},"gpuClass":"premium","kernelspec":{"display_name":"Python 3.10.4 ('yukinak')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.4"},"vscode":{"interpreter":{"hash":"b411586366dd8bdb0fc2800d6aef8fbd6cc14d2a48eec1b457375028a8621915"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"dc77b015b6cf4d709de4d623fb35ceee":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a6802706513241b2a0535ff81d8b7806","IPY_MODEL_d5d8583dffd54565a533d10f48982ff9","IPY_MODEL_c804627e0fb4487180d44e05d1f0b173"],"layout":"IPY_MODEL_65217be064ad49e6ab86f77eb90f4b54"}},"a6802706513241b2a0535ff81d8b7806":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e470c30f09754fd487031f7642d45713","placeholder":"​","style":"IPY_MODEL_6a48d8f78ea0463d96a69cdfef62d35f","value":"100%"}},"d5d8583dffd54565a533d10f48982ff9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b072bd725c64e0e82308e8d306a0be6","max":241669177,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2f31b70d35f4464b99f82e23229552ce","value":241669177}},"c804627e0fb4487180d44e05d1f0b173":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4301d22fe53b426e858fd743d828793e","placeholder":"​","style":"IPY_MODEL_2227bd076f454cbbb785905234bdb4f9","value":" 230M/230M [00:01&lt;00:00, 253MB/s]"}},"65217be064ad49e6ab86f77eb90f4b54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e470c30f09754fd487031f7642d45713":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a48d8f78ea0463d96a69cdfef62d35f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b072bd725c64e0e82308e8d306a0be6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f31b70d35f4464b99f82e23229552ce":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4301d22fe53b426e858fd743d828793e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2227bd076f454cbbb785905234bdb4f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}